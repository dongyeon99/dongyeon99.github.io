---
layout: post
title: Linux job scheduler "Slurm" 사용 및 관리
subtitle: 클러스터 Slurm 관리, 사용 하기
categories: [Linux]
tags: [Linux, Slurm, job scheduler]
comments: true
---

## 목차
#### 1. Job Script 만들기
#### 2. Job 제출, 확인, 삭제
#### 3. 환경 설정


<br/>

#### Slurm은 linux cluster 환경에서 사용되는 Schedular 중 하나입니다. slurm이 GPU schedule 관리 기능을 지원하기도 합니다.

<br/>

### - 기본명령어 요약



| Command                     |                       Explanation                         |
| :-------------------------: | :-------------------------------------------------------: |
| $ sbatch [옵션..] 스크립트   | 작업 제출                                                  |
| $ scancel 작업ID            | 작업 삭제                                                  |
| $ squeue                    | 작업 상태 확인                                             |
| $ smap                      | 작업 상태 및 노드 상태 확인                                 |
| $ sinfo [옵션..]            | 노드 정보 확인                                             |



<br/>

### - $ sbatch
작업 제출을 위한 command입니다. 사용방법 및 예시는 아래와 같습니다.

```
$ sbatch ./job_script.bash
```

### - Job Script (slurm) command 

```
#!/bin/sh

#SBATCH -J test            # 작업 이름
#SBATCH -p cas_v100_2      # partition 이름 작업 수행할 파티션을 지정하는 것이며, 사용가능한 파티션 이름은 sinfo 명령어로 확인이 가능
#SBATCH -N 2               # 총 필요한 컴퓨팅 노드 수, 작업 수행에 필요한 총 프로세스 수는 "-n" 옵션으로 정의
#SBATCH -n 2               # 총 필요한 프로세스 수, 작업 수행에 필요한 컴퓨팅 노드 개수는 "-N"으로 정의
#SBATCH -o %x.o%j          # stdout 파일 명(.o) Standard output을 저장할 파일명을 정의
#SBATCH -e %x.e%j          # stderr 파일 명(.e) Standard error 를 저장할 파일명을 정의
#SBATCH --comment          # Application별 SBATCH 옵션 (옵션명은 아래 작업스크립트 작성 안내 참고)
#SBATCH --time 00:30:00    # 최대 작업 시간 (Wall Time Clock Limit), 예상되는 작업 소요 시간을 의미, 실제 예상되는 작업 소요 시간보다 약간 더 길게 설정해 주는 것이 안전합니다. 만약 해당 작업이 설정해 둔 시간내에 종료 되지 않을 경우, Wall Time Clock Limit 시간을 초과하는 시점에서 slurm이 해당 작업을 강제로 종료시킵니다. 이 키워드는 작업 제출 스크립트 파일에 반드시 지정되어야 하며, 초단위 또는 시:분:초 형식으로 지정할 수 있습니다. Wall Time Clock Limit을 1시간을 지정할 경우, "--time=01:00:00: 또는 "-t 01:00:00" 과 같이 사용합니다.
#SBATCH --gres=gpu:2       # GPU를 사용하기 위한 옵션

srun ./run.x               # srun 사용
```

<br/>



<br/>

## 1. Job Script 만들기
slurm을 사용하기 전 수행할 job을 기록하는 job script를 만들어야 합니다. 이때 linux의 기본 Shell인 bash를 이용해 보았습니다.
bioinfomatics와 관련된 예시를 통해 파일을 만들어봅니다. samtools라는 tool을 이용해서 bam 파일을 sam 파일로 변환하는 작업을 해보겠습니다.

```
samtools view allignments.bam > alignments.sam
```

이를 bash script를 통해 만들어 준다면 다음과 같습니다.

```
#!/bin/bash
samtools view alignments.bam > alignments.sam
```

앞의 Script를 사용하여 작업을 제출해도 되지만, slurm Schedular에 넘겨줄 여러 옵션들을 script 파일 내용에 추가로 적어줄 수 있습니다. 
"#SBATCH" 이 그러한 기능을 하는 부분입니다. 이 부분은 # 으로 시작했기 때문에 bash 입장에서는 주석으로 여겨지므로 아무런 영향이 없고, slurm 에서는 #SBATCH 를 인식하여 이 부분은 수행할 작업에 대한 옵션으로 해석됩니다.

```
#!/bin/bash

#SBATCH -J samtools          # job name
#SBATCH -o samtools.%j.out   # standard output and error log

samtools view alignments.bam > alignments.sam
```

위와 같이 Script를 작성하여 samtools_script.sh 라는 파일을 만들었습니다. 
여기선 작업의 이름을 지정하는 "-J" 옵션과 job script의 출력 파일을 지정하는 "-o" 옵션을 사용했습니다.
job script에서 표준출력(standard output)이나 표준에러(standard error)가 나온다면 이를 저장할 수 있는데 이 때의 출력 파일 이름을 "-o" 옵션으로 지정해주었습니다. 
이름 중에 %j 라고 쓰인 부분은 작업이 제출되고 난 뒤에 부여되는 작업의 고유번호를 말합니다. 
만약 작업의 고유번호가 11로 부여되었다면 작업 스크립트의 출력은 samtools.11.out 이라는 파일에 저장될 것입니다.


<br/>

## 2. Job 제출, 확인, 삭제

### Job 제출
Slurm 에서 작업 제출(job submission)은 "sbatch" command를 통해 이루어집니다. 
sbatch 명령어의 첫 번째 인자에는 앞서 만든 작업 스크립트 파일을 넣어주면 됩니다. 
위에서 samtools_script.sh 라는 파일을 만들었으므로, 이를 이용하여 작업을 제출해 봅니다.

```
$ sbatch samtools_script.sh
Submitted batch job 11
```

작업이 문제 없이 제출이 되면 ‘Submitted batch job {JOBID}’ 메시지를 출력합니다. 
여기서 JOBID 는 제출된 작업에 부여된 식별 번호입니다. 
이는 작업을 제출할 때마다 1씩 증가합니다.

<br/>

### Job 확인
실행 중인 작업들의 목록을 확인하고 싶다면 "squeue" command를 이용합니다.

```
$ squeue

 JOBID  PARTITION         NAME USER  ST  TIME  NODES  NODELIST(REASON)
  11     normal    samtools_sc  lab  R   0:05      1  node1
```

작업들의 목록 테이블에 있는 각 필드들의 뜻은 다음과 같습니다.

```
JOBID : 제출한 작업의 식별 번호
PARTITION : 현재 작업이 제출된 파티션의 이름 (slurm에서의 partition은 SGE에서의 queue와 같은 개념)
NAME : 작업의 이름
USER : 작업을 제출한 리눅스 계정의 이름
ST : 현재 작업의 상태 (R: running, PD: pending)
NODELIST : 현재 이 작업을 수행할 수 있도록 할당된 컴퓨터 노드들
```

제출된 작업의 이름을 확인할 수 있는 필드인 NAME은 길이가 짧아 이름을 확인하기에는 어려움이 있습니다. 
이 부분을 해결할 수 있도록 squeue 명령어에 "formatting" 옵션을 주어 출력할 수 있습니다. 
아래의 formatting 옵션 중 %.50j 부분이 작업의 이름을 나타내는 필드의 가로 길이를 결정해 주는 부분입니다.

```
squeue --format="%.10i %.9P %.50j %.8u %.8T %.10M %.9l %.6D %R" | sort -n -k1,1
```

<br/>

### Job 삭제
실행 중인 Job을 중지시켜야 할 때 Job을 삭제하기 위해선 scancel 명령어와 취소할 작업 번호(JOBID)를 입력하면 됩니다.

```
$ scancel 11
```

<br/>

## 3. 환경 설정

### 파티션 목록 확인

Slurm에 존재하는 파티션의 목록들은 sinfo 명령어로 알 수 있습니다.

```
$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 2-00:00:00      2   idle node[1-2]
```

<br/>

### 파티션 생성
작업 파티션을 생성하기 위해서는 "scontrol create" 명령어를 이용합니다. 
관리자 권한이 필요합니다. 다음은 optim 라는 이름의 파티션을 생성하는 예시입니다.

```
$ sudo scontrol create PartitionName=optim
```

파티션 생성을 확인하기 위해 sinfo를 사용하면

```
$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
normal*      up 2-00:00:00      2   idle node[1-2]
optim        up   infinite      0    n/a
```

<br/>

### 파티션 세부 정보 확인
파티션을 생성하면 기본적으로 여러 속성들이 설정되는데, 기본적으로 설정되는 값들은 아래와 같습니다. 
scontrol show partition {partition_name} 명령을 이용하여 해당 파티션에 설정된 속성들을 확인할 수 있습니다.

```
$ scontrol show partition optim
PartitionName=optim
   AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
   AllocNodes=ALL Default=NO QoS=N/A
   DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO
   MaxNodes=UNLIMITED MaxTime=UNLIMITED MinNodes=1 LLN=NO MaxCPUsPerNode=UNLIMITED
   Nodes=(null)
   PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO
   OverTimeLimit=NONE PreemptMode=OFF
   State=UP TotalCPUs=0 TotalNodes=0 SelectTypeParameters=NONE
   JobDefaults=(null)
   DefMemPerNode=UNLIMITED MaxMemPerNode=UNLIMITED
```

이 상태로 파티션에 작업을 제출하면 작업이 실행되지 않고 대기중(PD, Pending)인 상태로 남아 있게 됩니다. 
파티션의 설정(PartitionConfig)이 아직 완성되지 않았기 때문입니다. 
이 파티션에 작업이 들어올 경우 "어느 노드에서 실행시켜야 하는지", "얼마만큼의 CPUs 를 할당할 수 있는지" 등의 정보들을 추가적으로 설정해 주어야 합니다.

```
$ squeue
    JOBID PARTITION         NAME   USER ST       TIME  NODES NODELIST(REASON)
     11    optipm   samtools_scr   dong PD       0:00      1 (PartitionConfig)
```

위에서 본 파티션에 설정된 속성들 중에서 추가/변경이 필요한 속성들은 아래와 같습니다.

```
Nodes=n[1-51]: 파티션에 연결되어 작동할 노드들의 목록. 여러 개일 경우 콤마로 구분.
MinNodes=1: 하나의 작업이 가질 수 있는 노드 수의 최소값.
MaxCPUsPerNode=4: 노드 당 가질 수 있는 CPUs 의 개수를 설정.
```

처음에 파티션을 만들 때부터 적어줘도 되지만, 그렇지 못하였다면 아래와 같이 scontrol update 명령으로 속성들을 추가해주거나, 수정해줄 수 있습니다.

```
sudo scontrol update PartitionName=optim NaxCPUsPerNode=10
```

지금까지의 파티션 설정 내용을 보면 아래와 같습니다.

```
$ scontrol show partition
PartitionName=optim
   AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
   AllocNodes=ALL Default=YES QoS=N/A
   DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO
   MaxNodes=UNLIMITED MaxTime=1-00:00:00 MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED
   Nodes=n[1-51]
   PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO
   OverTimeLimit=NONE PreemptMode=OFF
   State=UP TotalCPUs=204 TotalNodes=51 SelectTypeParameters=NONE
   JobDefaults=(null)
   DefMemPerNode=UNLIMITED MaxMemPerNode=UNLIMITED
```

<br/>

### 파티션 삭제

파티션을 삭제하려면 아래와 같이 입력하면 됩니다.

```
sudo scontrol delete PartitionName=optim
```

<br/>

### 기타 명령어

각 계산 노드들의 상태 확인

```
$ sinfo -N

NODELIST   NODES PARTITION STATE
n1             1  optim    down
n2             1  optim    down
n3             1  optim    down
...
n51            1  optim    down
node01         1    dongs  down
node02         1    dongs  down
```

Slurm 설치 여부 및 버전 확인

```
$ slurmctld -V
slurm 18.08.8
```
```
$ slurmd -V
slurm 18.08.8
```

<br/>

### 관련 경로

```
/etc/slurm/slurm.conf: slurm 의 환경 설정 파일
```

환경 설정 파일에는 여러가지 경로들이 설정되어 있다.

```
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
...
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdLogFile=/var/log/slurmd.log
```

```
/var/spool/slurm: 현재 돌고 있는 작업들의 스크립트가 저장되는 곳
```



